{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the code, make sure that this code, training data and test data are all in the same directory and the test data is named as \"test.csv\". This code takes around 10-15 minutes to run(excluding the time taken to download any library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>business</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>politics</td>\n",
       "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>politics</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>sport</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               text\n",
       "0              tech  tv future in the hands of viewers with home th...\n",
       "1          business  worldcom boss  left books alone  former worldc...\n",
       "2             sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3             sport  yeading face newcastle in fa cup premiership s...\n",
       "4     entertainment  ocean s twelve raids box office ocean s twelve...\n",
       "...             ...                                                ...\n",
       "2220       business  cars pull down us retail figures us retail sal...\n",
       "2221       politics  kilroy unveils immigration policy ex-chatshow ...\n",
       "2222  entertainment  rem announce new glasgow concert us band rem h...\n",
       "2223       politics  how political squabbles snowball it s become c...\n",
       "2224          sport  souness delight at euro progress boss graeme s...\n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will preprocess the data i.e remove those rows which contain empty entries, convert the word in text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   category  2225 non-null   object\n",
      " 1   text      2225 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 34.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = [entry.lower() for entry in data['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for categorical labels, we use label encoding to convert the labels into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "data['category'] = Encoder.fit_transform(data['category'])\n",
    "data1 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "3    511\n",
      "0    510\n",
      "2    417\n",
      "4    401\n",
      "1    386\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "category_counts = data['category'].value_counts()\n",
    "print(category_counts)\n",
    "\n",
    "#Since each class is in almost same amount, there is no need for balancing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we tokenize our dataset into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = [word_tokenize(entry) for entry in data['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       category                                               text\n",
       "0            4  [tv, future, in, the, hands, of, viewers, with...\n",
       "1            0  [worldcom, boss, left, books, alone, former, w...\n",
       "2            3  [tigers, wary, of, farrell, gamble, leicester,...\n",
       "3            3  [yeading, face, newcastle, in, fa, cup, premie...\n",
       "4            1  [ocean, s, twelve, raids, box, office, ocean, ...\n",
       "...        ...                                                ...\n",
       "2220         0  [cars, pull, down, us, retail, figures, us, re...\n",
       "2221         2  [kilroy, unveils, immigration, policy, ex-chat...\n",
       "2222         1  [rem, announce, new, glasgow, concert, us, ban...\n",
       "2223         2  [how, political, squabbles, snowball, it, s, b...\n",
       "2224         3  [souness, delight, at, euro, progress, boss, g...\n",
       "\n",
       "[2225 rows x 2 columns]>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is lemmatizing so as to reduce the words into their root form and in the mean process, we also remove stop-words and numbers in the for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x000001544F9696C0>, {'J': 'a', 'V': 'v', 'R': 'r'})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>[tv, future, in, the, hands, of, viewers, with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[worldcom, boss, left, books, alone, former, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[tigers, wary, of, farrell, gamble, leicester,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[yeading, face, newcastle, in, fa, cup, premie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[ocean, s, twelve, raids, box, office, ocean, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>0</td>\n",
       "      <td>[cars, pull, down, us, retail, figures, us, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>2</td>\n",
       "      <td>[kilroy, unveils, immigration, policy, ex-chat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>1</td>\n",
       "      <td>[rem, announce, new, glasgow, concert, us, ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>2</td>\n",
       "      <td>[how, political, squabbles, snowball, it, s, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>3</td>\n",
       "      <td>[souness, delight, at, euro, progress, boss, g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                               text\n",
       "0            4  [tv, future, in, the, hands, of, viewers, with...\n",
       "1            0  [worldcom, boss, left, books, alone, former, w...\n",
       "2            3  [tigers, wary, of, farrell, gamble, leicester,...\n",
       "3            3  [yeading, face, newcastle, in, fa, cup, premie...\n",
       "4            1  [ocean, s, twelve, raids, box, office, ocean, ...\n",
       "...        ...                                                ...\n",
       "2220         0  [cars, pull, down, us, retail, figures, us, re...\n",
       "2221         2  [kilroy, unveils, immigration, policy, ex-chat...\n",
       "2222         1  [rem, announce, new, glasgow, concert, us, ban...\n",
       "2223         2  [how, political, squabbles, snowball, it, s, b...\n",
       "2224         3  [souness, delight, at, euro, progress, boss, g...\n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tag_map)\n",
    "word_Lemmatized = WordNetLemmatizer()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(data['text']):\n",
    "    Final_words = []\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word, pos = tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    data.loc[index,'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>text_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>[tv, future, in, the, hands, of, viewers, with...</td>\n",
       "      <td>['tv', 'future', 'hand', 'viewer', 'home', 'th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[worldcom, boss, left, books, alone, former, w...</td>\n",
       "      <td>['worldcom', 'bos', 'leave', 'book', 'alone', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[tigers, wary, of, farrell, gamble, leicester,...</td>\n",
       "      <td>['tiger', 'wary', 'farrell', 'gamble', 'leices...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[yeading, face, newcastle, in, fa, cup, premie...</td>\n",
       "      <td>['yeading', 'face', 'newcastle', 'fa', 'cup', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[ocean, s, twelve, raids, box, office, ocean, ...</td>\n",
       "      <td>['ocean', 'twelve', 'raid', 'box', 'office', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>0</td>\n",
       "      <td>[cars, pull, down, us, retail, figures, us, re...</td>\n",
       "      <td>['car', 'pull', 'u', 'retail', 'figure', 'u', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>2</td>\n",
       "      <td>[kilroy, unveils, immigration, policy, ex-chat...</td>\n",
       "      <td>['kilroy', 'unveils', 'immigration', 'policy',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>1</td>\n",
       "      <td>[rem, announce, new, glasgow, concert, us, ban...</td>\n",
       "      <td>['rem', 'announce', 'new', 'glasgow', 'concert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>2</td>\n",
       "      <td>[how, political, squabbles, snowball, it, s, b...</td>\n",
       "      <td>['political', 'squabble', 'snowball', 'become'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>3</td>\n",
       "      <td>[souness, delight, at, euro, progress, boss, g...</td>\n",
       "      <td>['souness', 'delight', 'euro', 'progress', 'bo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                               text  \\\n",
       "0            4  [tv, future, in, the, hands, of, viewers, with...   \n",
       "1            0  [worldcom, boss, left, books, alone, former, w...   \n",
       "2            3  [tigers, wary, of, farrell, gamble, leicester,...   \n",
       "3            3  [yeading, face, newcastle, in, fa, cup, premie...   \n",
       "4            1  [ocean, s, twelve, raids, box, office, ocean, ...   \n",
       "...        ...                                                ...   \n",
       "2220         0  [cars, pull, down, us, retail, figures, us, re...   \n",
       "2221         2  [kilroy, unveils, immigration, policy, ex-chat...   \n",
       "2222         1  [rem, announce, new, glasgow, concert, us, ban...   \n",
       "2223         2  [how, political, squabbles, snowball, it, s, b...   \n",
       "2224         3  [souness, delight, at, euro, progress, boss, g...   \n",
       "\n",
       "                                             text_final  \n",
       "0     ['tv', 'future', 'hand', 'viewer', 'home', 'th...  \n",
       "1     ['worldcom', 'bos', 'leave', 'book', 'alone', ...  \n",
       "2     ['tiger', 'wary', 'farrell', 'gamble', 'leices...  \n",
       "3     ['yeading', 'face', 'newcastle', 'fa', 'cup', ...  \n",
       "4     ['ocean', 'twelve', 'raid', 'box', 'office', '...  \n",
       "...                                                 ...  \n",
       "2220  ['car', 'pull', 'u', 'retail', 'figure', 'u', ...  \n",
       "2221  ['kilroy', 'unveils', 'immigration', 'policy',...  \n",
       "2222  ['rem', 'announce', 'new', 'glasgow', 'concert...  \n",
       "2223  ['political', 'squabble', 'snowball', 'become'...  \n",
       "2224  ['souness', 'delight', 'euro', 'progress', 'bo...  \n",
       "\n",
       "[2225 rows x 3 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(data['text_final'], data['category'], test_size = 0.3, random_state=75)\n",
    "#Splitting the dataset into training and validation (testing) set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to extract the features, I will use 2 methods and compare their accuracies:\n",
    "1) Tf-Idf - Term Freqeuncy-Inverse Document Frequency\n",
    "2) word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=data['text'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to average word vectors for each document\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            n_words += 1\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    if n_words:\n",
    "        feature_vector = np.divide(feature_vector, n_words)\n",
    "    return feature_vector\n",
    "\n",
    "# Get Word2Vec vocabulary\n",
    "w2v_vocab = set(word2vec_model.wv.index_to_key)\n",
    "\n",
    "# Convert train and test sets to averaged Word2Vec vectors\n",
    "train_x_w2v = [average_word_vectors(words, word2vec_model, w2v_vocab, 100) for words in train_x]\n",
    "test_x_w2v = [average_word_vectors(words, word2vec_model, w2v_vocab, 100) for words in test_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_features=10000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=10000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(max_features=10000)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=10000)\n",
    "Tfidf_vect.fit(data['text_final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_Tfidf = Tfidf_vect.transform(train_x)\n",
    "test_x_Tfidf = Tfidf_vect.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting the useful tokens, now we implement various models for the purpose of training the model for multiclass text classification and note their accuracies.\n",
    "Various architectures which we will see are:\n",
    "1) Decision Tree\n",
    "2) Logistic Regression\n",
    "3) Naive Bayes\n",
    "4) Linear SVM\n",
    "5) Deep Learning techniques with LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy Score ->  83.53293413173652\n"
     ]
    }
   ],
   "source": [
    "# Create and train the Decision Tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(train_x_Tfidf, train_y)\n",
    "\n",
    "# Predict labels on the test set\n",
    "pred_DT = clf.predict(test_x_Tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_DT = accuracy_score(test_y, pred_DT)\n",
    "print(\"Decision Tree Accuracy Score -> \", accuracy_DT * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "39/39 [==============================] - 3s 48ms/step - loss: 1.6020 - accuracy: 0.2586 - val_loss: 1.5787 - val_accuracy: 0.3429\n",
      "Epoch 2/8\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 1.2928 - accuracy: 0.4602 - val_loss: 1.4701 - val_accuracy: 0.3205\n",
      "Epoch 3/8\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 1.0372 - accuracy: 0.5855 - val_loss: 0.8220 - val_accuracy: 0.6250\n",
      "Epoch 4/8\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.5477 - accuracy: 0.8096 - val_loss: 0.6099 - val_accuracy: 0.7853\n",
      "Epoch 5/8\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2960 - accuracy: 0.9221 - val_loss: 0.5042 - val_accuracy: 0.8237\n",
      "Epoch 6/8\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1039 - accuracy: 0.9839 - val_loss: 0.9351 - val_accuracy: 0.7981\n",
      "Epoch 7/8\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.0914 - accuracy: 0.9863 - val_loss: 0.5173 - val_accuracy: 0.8910\n",
      "Epoch 8/8\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0302 - accuracy: 0.9976 - val_loss: 0.5443 - val_accuracy: 0.8622\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3936 - accuracy: 0.8772\n",
      "Test Accuracy with DNN: 0.8772454857826233\n"
     ]
    }
   ],
   "source": [
    "X = data1['text']\n",
    "y = data1['category']\n",
    "\n",
    "# Tokenization and sequence padding\n",
    "tokenizer = Tokenizer(num_words=10000)  # Assuming a vocabulary size of 10,000\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(sequences, maxlen=100)  # Assuming a maximum sequence length of 100\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=75)\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 128, input_length=100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(Encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=8, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy with DNN: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  96.55688622754491\n"
     ]
    }
   ],
   "source": [
    "NB = naive_bayes.MultinomialNB()\n",
    "NB.fit(train_x_Tfidf,train_y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "pred_NB = NB.predict(test_x_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(pred_NB, test_y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy (Tf-Idf)-> 97.75449101796407\n",
      "Logistic Regression Accuracy Score (Word2Vec) ->  35.778443113772454\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "logistic_reg = LogisticRegression(max_iter=1000)  # Initialize Logistic Regression\n",
    "logistic_reg.fit(train_x_Tfidf, train_y)  # Train the model\n",
    "\n",
    "# Predictions\n",
    "y_pred_tfidf = logistic_reg.predict(test_x_Tfidf)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(test_y, y_pred_tfidf)\n",
    "print(f\"Logistic Regression Accuracy (Tf-Idf)-> {accuracy * 100}\")\n",
    "\n",
    "# Initialize and train Logistic Regression using Word2Vec representations\n",
    "logistic_reg_w2v = LogisticRegression(max_iter=1000)\n",
    "logistic_reg_w2v.fit(train_x_w2v, train_y)\n",
    "\n",
    "# Predict on the test set using Word2Vec representations\n",
    "pred_logistic_w2v = logistic_reg_w2v.predict(test_x_w2v)\n",
    "\n",
    "# Calculate accuracy using accuracy_score\n",
    "accuracy_logistic_w2v = accuracy_score(pred_logistic_w2v, test_y)\n",
    "print(\"Logistic Regression Accuracy Score (Word2Vec) -> \", accuracy_logistic_w2v * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics (Tf-Idf):\n",
      "Accuracy: 97.75449101796407%\n",
      "Precision: 0.9778055994182709\n",
      "Recall: 0.9775449101796407\n",
      "F1-score: 0.9775462856918494\n",
      "\n",
      "Logistic Regression Metrics (Word2Vec):\n",
      "Accuracy: 35.778443113772454%\n",
      "Precision: 0.36702628047354613\n",
      "Recall: 0.35778443113772457\n",
      "F1-score: 0.3367197035533312\n"
     ]
    }
   ],
   "source": [
    "accuracy_tfidf = accuracy_score(test_y, y_pred_tfidf)\n",
    "precision_tfidf = precision_score(test_y, y_pred_tfidf, average='weighted')\n",
    "recall_tfidf = recall_score(test_y, y_pred_tfidf, average='weighted')\n",
    "f1_tfidf = f1_score(test_y, y_pred_tfidf, average='weighted')\n",
    "\n",
    "print(\"Logistic Regression Metrics (Tf-Idf):\")\n",
    "print(f\"Accuracy: {accuracy_tfidf * 100:}%\")\n",
    "print(f\"Precision: {precision_tfidf:}\")\n",
    "print(f\"Recall: {recall_tfidf:}\")\n",
    "print(f\"F1-score: {f1_tfidf:}\")\n",
    "\n",
    "accuracy_word2vec = accuracy_score(test_y, pred_logistic_w2v)\n",
    "precision_word2vec = precision_score(test_y, pred_logistic_w2v, average='weighted')\n",
    "recall_word2vec = recall_score(test_y, pred_logistic_w2v, average='weighted')\n",
    "f1_word2vec = f1_score(test_y, pred_logistic_w2v, average='weighted')\n",
    "\n",
    "print(\"\\nLogistic Regression Metrics (Word2Vec):\")\n",
    "print(f\"Accuracy: {accuracy_word2vec * 100:}%\")\n",
    "print(f\"Precision: {precision_word2vec:}\")\n",
    "print(f\"Recall: {recall_word2vec:}\")\n",
    "print(f\"F1-score: {f1_word2vec:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = svm.SVC(C = 1, kernel = 'linear', degree = 3, gamma = 'auto')\n",
    "SVM.fit(train_x_Tfidf, train_y)\n",
    "pred_SVM = SVM.predict(test_x_Tfidf)\n",
    "\n",
    "# Save the SVM model with (tf-idf feature engineering) to a file\n",
    "joblib.dump(SVM, 'final_model.pkl')\n",
    "\n",
    "\n",
    "# Initialize and train SVM using Word2Vec representations\n",
    "svm_classifier = svm.SVC(C = 1, kernel = 'linear', degree = 3, gamma = 'auto')  # You can choose different kernels like 'rbf' or 'poly'\n",
    "svm_classifier.fit(train_x_w2v, train_y)\n",
    "\n",
    "# Predict on the test set using Word2Vec representations\n",
    "pred_svm = svm_classifier.predict(test_x_w2v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Metrics (Tf-Idf):\n",
      "Accuracy: 98.80239520958084%\n",
      "Precision: 0.9881159949253597\n",
      "Recall: 0.9880239520958084\n",
      "F1-score: 0.9880320996705151\n",
      "\n",
      "SVM Metrics (Word2Vec):\n",
      "Accuracy: 31.58682634730539%\n",
      "Precision: 0.6575845930235742\n",
      "Recall: 0.3158682634730539\n",
      "F1-score: 0.4073679805725062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate SVM model trained on Tf-Idf representations\n",
    "accuracy_tfidf = accuracy_score(pred_SVM, test_y)\n",
    "precision_tfidf = precision_score(pred_SVM, test_y, average='weighted')\n",
    "recall_tfidf = recall_score(pred_SVM, test_y, average='weighted')\n",
    "f1_tfidf = f1_score(pred_SVM, test_y, average='weighted')\n",
    "\n",
    "print(\"SVM Metrics (Tf-Idf):\")\n",
    "print(f\"Accuracy: {accuracy_tfidf * 100:}%\")\n",
    "print(f\"Precision: {precision_tfidf:}\")\n",
    "print(f\"Recall: {recall_tfidf:}\")\n",
    "print(f\"F1-score: {f1_tfidf:}\")\n",
    "\n",
    "# Evaluate SVM model trained on Word2Vec representations\n",
    "accuracy_word2vec = accuracy_score(pred_svm, test_y)\n",
    "precision_word2vec = precision_score(pred_svm, test_y, average='weighted')\n",
    "recall_word2vec = recall_score(pred_svm, test_y, average='weighted')\n",
    "f1_word2vec = f1_score(pred_svm, test_y, average='weighted')\n",
    "\n",
    "print(\"\\nSVM Metrics (Word2Vec):\")\n",
    "print(f\"Accuracy: {accuracy_word2vec * 100:}%\")\n",
    "print(f\"Precision: {precision_word2vec:}\")\n",
    "print(f\"Recall: {recall_word2vec:}\")\n",
    "print(f\"F1-score: {f1_word2vec:}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, out of all the architectures tried above, Linear SVM performs the best. Also, we can see that Tf-Idf performs much better than word2vec. Hence, our final model will be Linear SVM with Tf-Idf feature extraction method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will evaluate the final model trained on train data on the test data. Just place the test data in the same directory as that of code and train data and make sure that it's named as \"test.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the test data in the same way as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['text'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['text'] = [entry.lower() for entry in test_data['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same encoder as that of training data\n",
    "test_data['category'] = Encoder.transform(test_data['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['text'] = [word_tokenize(entry) for entry in test_data['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(test_data['text']):\n",
    "    Final_words = []\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word, pos = tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    test_data.loc[index,'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the preprocessed test data using the trained TF-IDF vectorizer\n",
    "test_x = Tfidf_vect.transform(test_data['text_final'])\n",
    "test_y = test_data['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved SVM model from file\n",
    "loaded_SVM = joblib.load('final_model.pkl')\n",
    "predictions = loaded_SVM.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Metrics on Test Data:\n",
      "Accuracy: 99.59550561797754%\n",
      "Precision: 0.9959614752970547\n",
      "Recall: 0.9959550561797753\n",
      "F1-score: 0.9959555264488154\n"
     ]
    }
   ],
   "source": [
    "# Evaluate SVM model on test data\n",
    "accuracy_tfidf = accuracy_score(predictions, test_y)\n",
    "precision_tfidf = precision_score(predictions, test_y, average='weighted')\n",
    "recall_tfidf = recall_score(predictions, test_y, average='weighted')\n",
    "f1_tfidf = f1_score(predictions, test_y, average='weighted')\n",
    "\n",
    "print(\"SVM Metrics on Test Data:\")\n",
    "print(f\"Accuracy: {accuracy_tfidf * 100:}%\")\n",
    "print(f\"Precision: {precision_tfidf:}\")\n",
    "print(f\"Recall: {recall_tfidf:}\")\n",
    "print(f\"F1-score: {f1_tfidf:}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
